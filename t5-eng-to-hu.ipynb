{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"import-libraries","cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\nfrom datasets import load_dataset\nimport optuna\nimport random\nimport numpy as np\n\n# For reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(42)\n\nprint(f\"Using device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T11:53:04.985664Z","iopub.execute_input":"2025-03-14T11:53:04.986030Z","iopub.status.idle":"2025-03-14T11:53:27.940872Z","shell.execute_reply.started":"2025-03-14T11:53:04.986005Z","shell.execute_reply":"2025-03-14T11:53:27.940075Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":1},{"id":"load-dataset","cell_type":"code","source":"# Load the English-Hungarian translation dataset from Hugging Face\ndataset = load_dataset('opus_books', 'en-hu')\nprint(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T11:53:27.941872Z","iopub.execute_input":"2025-03-14T11:53:27.942717Z","iopub.status.idle":"2025-03-14T11:53:33.292229Z","shell.execute_reply.started":"2025-03-14T11:53:27.942677Z","shell.execute_reply":"2025-03-14T11:53:33.291361Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/28.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96c251de2118427db0a655cd73b86ab8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/23.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd95ab7ca11d4b748beaddfb0738fe1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/137151 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"139797858eb14be9b60a160a0aea7c30"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'translation'],\n        num_rows: 137151\n    })\n})\n","output_type":"stream"}],"execution_count":2},{"id":"83aa2d1b","cell_type":"code","source":"from datasets import Dataset\n\n# Extract the translation list from your dataset\ntranslation_list = dataset[\"train\"][\"translation\"]\nhf_dataset = Dataset.from_list(translation_list)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T11:53:33.293658Z","iopub.execute_input":"2025-03-14T11:53:33.293952Z","iopub.status.idle":"2025-03-14T11:53:34.581118Z","shell.execute_reply.started":"2025-03-14T11:53:33.293929Z","shell.execute_reply":"2025-03-14T11:53:34.580472Z"}},"outputs":[],"execution_count":3},{"id":"preprocess-data","cell_type":"code","source":"# Initialize model name and tokenizer\nmodel_name = 't5-small'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Preprocessing function to add the T5 task prefix and tokenize both inputs and targets\ndef preprocess_function(examples):\n    inputs = [\"translate English to Hungarian: \" + ex for ex in examples[\"en\"]]\n    targets = [ex for ex in examples[\"hu\"]]\n\n    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding='max_length')\n    labels = tokenizer(targets, max_length=128, truncation=True, padding='max_length')\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Tokenize the train and validation splits  \ntokenized_datasets = hf_dataset.map(preprocess_function, batched=True)\n\n# Set the dataset format to PyTorch tensors\ntokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n\nprint(tokenized_datasets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T11:53:34.582442Z","iopub.execute_input":"2025-03-14T11:53:34.582773Z","iopub.status.idle":"2025-03-14T11:54:05.748543Z","shell.execute_reply.started":"2025-03-14T11:53:34.582742Z","shell.execute_reply":"2025-03-14T11:54:05.747675Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a34888eb76714f829f5c4557b8d07837"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e793761e7ef45329afd642c0303ab09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6e7ae5520b645d98161f2f98ee5bfcd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/137151 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb747a9166d24205b65b1de0f8279052"}},"metadata":{}},{"name":"stdout","text":"Dataset({\n    features: ['en', 'hu', 'input_ids', 'attention_mask', 'labels'],\n    num_rows: 137151\n})\n","output_type":"stream"}],"execution_count":4},{"id":"optuna-objective","cell_type":"code","source":"import optuna\nimport torch\nfrom transformers import TrainingArguments, Trainer, T5ForConditionalGeneration\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\n\ndef objective(trial):\n    # Suggest hyperparameters\n    learning_rate = trial.suggest_float(\"learning_rate\", 5e-5, 1e-3, log=True)  # Slightly wider range\n    warmup_steps = trial.suggest_int(\"warmup_steps\", 500, 1500, step=500)  # Add warmup tuning\n    gradient_accumulation_steps = trial.suggest_int(\"gradient_accumulation_steps\", 2, 8)\n    weight_decay = trial.suggest_float(\"weight_decay\", 0.0, 0.3)\n    per_device_train_batch_size = trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16, 32])\n    num_train_epochs = 1  # Use 1 epoch for faster tuning\n\n    training_args = TrainingArguments(\n        output_dir='./t5-small-translation-optuna',\n        evaluation_strategy='steps',\n        eval_steps=500,\n        per_device_train_batch_size=per_device_train_batch_size,\n        per_device_eval_batch_size=per_device_train_batch_size,\n        learning_rate=learning_rate,\n        num_train_epochs=num_train_epochs,\n        weight_decay=weight_decay,\n        report_to=\"tensorboard\",  # ðŸ”¥ Logs to TensorBoard\n        logging_dir=\"./t5-small-translation-tensorboard\",  # ðŸ”¥ TensorBoard log dir\n        gradient_accumulation_steps=gradient_accumulation_steps,\n        warmup_steps=warmup_steps,  # Add tuned warmup\n        lr_scheduler_type=\"cosine_with_restarts\",  # Use cosine scheduler\n        logging_steps=100,\n        save_steps=500,\n        save_total_limit=2,\n        fp16=torch.cuda.is_available(),  # Enable mixed precision training if GPU available\n        load_best_model_at_end=True,\n        metric_for_best_model='loss',\n        dataloader_num_workers=4\n    )\n    \n    # Reinitialize model for each trial\n    model = T5ForConditionalGeneration.from_pretrained(model_name)\n    \n    # Split using Hugging Face's built-in method\n    train_test_split = tokenized_datasets.train_test_split(test_size=0.2)\n    train_data = train_test_split[\"train\"]\n    eval_data = train_test_split[\"test\"]\n\n    \n    train_dataset = train_data.remove_columns([\"en\", \"hu\"])\n    eval_dataset = eval_data.remove_columns([\"en\", \"hu\"])\n\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        tokenizer=tokenizer,\n    )\n    \n    trainer.train()\n    eval_result = trainer.evaluate()\n    \n    # Return the evaluation loss as the objective metric\n    return eval_result[\"eval_loss\"]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T11:54:05.749337Z","iopub.execute_input":"2025-03-14T11:54:05.749554Z","iopub.status.idle":"2025-03-14T11:54:05.768468Z","shell.execute_reply.started":"2025-03-14T11:54:05.749535Z","shell.execute_reply":"2025-03-14T11:54:05.767558Z"}},"outputs":[],"execution_count":5},{"id":"run-optuna","cell_type":"code","source":"# Run the Optuna study\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)\n\nprint(\"Best trial:\")\ntrial = study.best_trial\nprint(f\"  Value: {trial.value}\")\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(f\"    {key}: {value}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T11:54:05.769416Z","iopub.execute_input":"2025-03-14T11:54:05.769746Z","iopub.status.idle":"2025-03-14T14:43:57.973463Z","shell.execute_reply.started":"2025-03-14T11:54:05.769714Z","shell.execute_reply":"2025-03-14T14:43:57.971954Z"}},"outputs":[{"name":"stderr","text":"[I 2025-03-14 11:54:05,772] A new study created in memory with name: no-name-09bae40b-4fb6-46a6-ac93-a97b41a8a821\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c2bdc2593584b3a89622b35a5ea48f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0765093f22cc45d0a241968f00966868"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"877db231a9874da7906b22e10b3fd73d"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-5-77328bfd3527>:52: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='489' max='489' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [489/489 17:26, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='858' max='858' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [858/858 01:31]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2025-03-14 12:13:10,088] Trial 0 finished with value: 1.2588436603546143 and parameters: {'learning_rate': 0.0005708948855396893, 'warmup_steps': 500, 'gradient_accumulation_steps': 7, 'weight_decay': 0.10451249398574737, 'per_device_train_batch_size': 32}. Best is trial 0 with value: 1.2588436603546143.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-5-77328bfd3527>:52: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4571' max='4571' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4571/4571 39:07, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>1.669800</td>\n      <td>1.527621</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.404200</td>\n      <td>1.315560</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>1.335500</td>\n      <td>1.222139</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.290000</td>\n      <td>1.162906</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>1.238700</td>\n      <td>1.124129</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>1.213700</td>\n      <td>1.098963</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>1.185700</td>\n      <td>1.084875</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>1.192300</td>\n      <td>1.077930</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>1.171000</td>\n      <td>1.076301</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3429' max='3429' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3429/3429 01:50]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2025-03-14 12:54:11,507] Trial 1 finished with value: 1.0763005018234253 and parameters: {'learning_rate': 0.0004378721591873564, 'warmup_steps': 500, 'gradient_accumulation_steps': 3, 'weight_decay': 0.021056370062719286, 'per_device_train_batch_size': 8}. Best is trial 1 with value: 1.0763005018234253.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-5-77328bfd3527>:52: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1143' max='1143' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1143/1143 20:41, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>1.533400</td>\n      <td>1.364499</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.289900</td>\n      <td>1.182360</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='858' max='858' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [858/858 01:31]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2025-03-14 13:16:27,203] Trial 2 finished with value: 1.1823604106903076 and parameters: {'learning_rate': 0.000496857824541632, 'warmup_steps': 500, 'gradient_accumulation_steps': 3, 'weight_decay': 0.22338282439705454, 'per_device_train_batch_size': 32}. Best is trial 1 with value: 1.0763005018234253.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-5-77328bfd3527>:52: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1143' max='1143' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1143/1143 20:41, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>1.865700</td>\n      <td>1.702902</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.640100</td>\n      <td>1.534465</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='858' max='858' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [858/858 01:31]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2025-03-14 13:38:42,638] Trial 3 finished with value: 1.5344651937484741 and parameters: {'learning_rate': 8.171934845166888e-05, 'warmup_steps': 500, 'gradient_accumulation_steps': 3, 'weight_decay': 0.05917915123627517, 'per_device_train_batch_size': 32}. Best is trial 1 with value: 1.0763005018234253.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-5-77328bfd3527>:52: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6857' max='6857' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6857/6857 46:51, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>1.962500</td>\n      <td>1.864804</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.809300</td>\n      <td>1.664031</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>1.675700</td>\n      <td>1.568769</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.652800</td>\n      <td>1.511431</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>1.584800</td>\n      <td>1.468552</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>1.575300</td>\n      <td>1.437542</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>1.562400</td>\n      <td>1.415097</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>1.486900</td>\n      <td>1.397786</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>1.500800</td>\n      <td>1.385230</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>1.499600</td>\n      <td>1.377480</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>1.493600</td>\n      <td>1.371375</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>1.498600</td>\n      <td>1.369445</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>1.437200</td>\n      <td>1.368390</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3429' max='3429' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3429/3429 01:50]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2025-03-14 14:27:26,671] Trial 4 finished with value: 1.3683903217315674 and parameters: {'learning_rate': 7.528660668458725e-05, 'warmup_steps': 500, 'gradient_accumulation_steps': 2, 'weight_decay': 0.22688529693890694, 'per_device_train_batch_size': 8}. Best is trial 1 with value: 1.0763005018234253.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-5-77328bfd3527>:52: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='970' max='1143' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 970/1143 16:28 < 02:56, 0.98 it/s, Epoch 0.85/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>1.934500</td>\n      <td>1.776820</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"[W 2025-03-14 14:43:57,621] Trial 5 failed with parameters: {'learning_rate': 0.00011022785282886311, 'warmup_steps': 1000, 'gradient_accumulation_steps': 3, 'weight_decay': 0.06172239734987018, 'per_device_train_batch_size': 32} because of the following error: KeyboardInterrupt().\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n  File \"<ipython-input-5-77328bfd3527>\", line 60, in objective\n    trainer.train()\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2164, in train\n    return inner_training_loop(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2522, in _inner_training_loop\n    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3688, in training_step\n    self.accelerator.backward(loss, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 2244, in backward\n    self.scaler.scale(loss).backward(**kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 581, in backward\n    torch.autograd.backward(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n    _engine_run_backward(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nKeyboardInterrupt\n[W 2025-03-14 14:43:57,626] Trial 5 failed with value None.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-23961999148c>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run the Optuna study\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'minimize'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best trial:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-77328bfd3527>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     58\u001b[0m     )\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0meval_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2164\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2165\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2166\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2520\u001b[0m                     )\n\u001b[1;32m   2521\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2522\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2524\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3686\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3687\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3688\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3689\u001b[0m             \u001b[0;31m# Finally we need to normalize the loss for reporting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3690\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2242\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2243\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2244\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2245\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_lomo_optimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2246\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":6},{"id":"5c44a895-19f0-4e26-894d-8913294d257b","cell_type":"code","source":"# Train the final model using the best hyperparameters found by Optuna\nbest_params = study.best_trial.params\nbest_params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T14:44:44.251036Z","iopub.execute_input":"2025-03-14T14:44:44.251349Z","iopub.status.idle":"2025-03-14T14:44:44.257211Z","shell.execute_reply.started":"2025-03-14T14:44:44.251323Z","shell.execute_reply":"2025-03-14T14:44:44.256423Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'learning_rate': 0.0004378721591873564,\n 'warmup_steps': 500,\n 'gradient_accumulation_steps': 3,\n 'weight_decay': 0.021056370062719286,\n 'per_device_train_batch_size': 8}"},"metadata":{}}],"execution_count":7},{"id":"final-training","cell_type":"code","source":"\n\nfinal_training_args = TrainingArguments(\n    output_dir='./t5-small-translation-final',\n    evaluation_strategy='steps',\n    eval_steps=500,\n    per_device_train_batch_size=best_params[\"per_device_train_batch_size\"],\n    per_device_eval_batch_size=best_params[\"per_device_train_batch_size\"],\n    learning_rate=best_params[\"learning_rate\"],\n    num_train_epochs=3,  # Train for more epochs on final run\n    weight_decay=best_params[\"weight_decay\"],\n    gradient_accumulation_steps=best_params[\"gradient_accumulation_steps\"],\n    logging_steps=100,\n    save_steps=500,\n    save_total_limit=2,\n    fp16=torch.cuda.is_available(),\n    load_best_model_at_end=True,\n    metric_for_best_model='loss',\n    dataloader_num_workers=4,\n    report_to=\"tensorboard\",  # ðŸ”¥ Logs to TensorBoard\n    logging_dir=\"./t5-small-translation-tensorboard\",  # ðŸ”¥ TensorBoard log dir\n)\n\n# Reinitialize the final model\nfinal_model = T5ForConditionalGeneration.from_pretrained(model_name)\n\n# Split the dataset again for final training\ntrain_test_split = tokenized_datasets.train_test_split(test_size=0.2)\ntrain_data = train_test_split[\"train\"]\neval_data = train_test_split[\"test\"]\n\n# Remove unnecessary columns\ntrain_dataset = train_data.remove_columns([\"en\", \"hu\"])\neval_dataset = eval_data.remove_columns([\"en\", \"hu\"])\n\nfinal_trainer = Trainer(\n    model=final_model,\n    args=final_training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=tokenizer\n)\n\nfinal_trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T14:44:52.875938Z","iopub.execute_input":"2025-03-14T14:44:52.876227Z","iopub.status.idle":"2025-03-14T16:42:13.404929Z","shell.execute_reply.started":"2025-03-14T14:44:52.876205Z","shell.execute_reply":"2025-03-14T16:42:13.404018Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-8-84c0047704be>:34: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  final_trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='13713' max='13713' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [13713/13713 1:57:18, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>1.550900</td>\n      <td>1.425730</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.365000</td>\n      <td>1.280305</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>1.314900</td>\n      <td>1.202585</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.273600</td>\n      <td>1.148224</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>1.221100</td>\n      <td>1.106241</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>1.189300</td>\n      <td>1.075608</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>1.150300</td>\n      <td>1.048828</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>1.139800</td>\n      <td>1.028209</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>1.099600</td>\n      <td>1.006354</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>1.037400</td>\n      <td>0.990276</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>1.064200</td>\n      <td>0.974364</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>1.046600</td>\n      <td>0.960823</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>1.035500</td>\n      <td>0.949791</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>1.073900</td>\n      <td>0.939774</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>1.015600</td>\n      <td>0.931605</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>1.016300</td>\n      <td>0.922057</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>1.010200</td>\n      <td>0.914924</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>1.004600</td>\n      <td>0.909583</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>0.993500</td>\n      <td>0.902116</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.999100</td>\n      <td>0.897191</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>0.966500</td>\n      <td>0.893435</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>0.983100</td>\n      <td>0.890126</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>0.980700</td>\n      <td>0.887349</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>0.971700</td>\n      <td>0.883957</td>\n    </tr>\n    <tr>\n      <td>12500</td>\n      <td>0.948500</td>\n      <td>0.881689</td>\n    </tr>\n    <tr>\n      <td>13000</td>\n      <td>0.975400</td>\n      <td>0.879961</td>\n    </tr>\n    <tr>\n      <td>13500</td>\n      <td>0.959100</td>\n      <td>0.878941</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=13713, training_loss=1.1034292878133067, metrics={'train_runtime': 7039.1568, 'train_samples_per_second': 46.761, 'train_steps_per_second': 1.948, 'total_flos': 1.1136194108719104e+16, 'train_loss': 1.1034292878133067, 'epoch': 2.999854174261757})"},"metadata":{}}],"execution_count":8},{"id":"evaluate-model","cell_type":"code","source":"# Define a helper function to translate text using the final model\ndef translate_text(text):\n    input_text = \"translate English to Hungarian: \" + text\n    input_ids = tokenizer(input_text, return_tensors='pt', max_length=128, truncation=True).input_ids\n    \n    if torch.cuda.is_available():\n        input_ids = input_ids.to('cuda')\n        final_model.to('cuda')\n    \n    outputs = final_model.generate(\n        input_ids, \n        max_length=128, \n        num_beams=4, \n        early_stopping=True\n    )\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Test translation with a sample sentence\nsample_text = \"I love books.\"\ntranslated_text = translate_text(sample_text)\n\nprint(f\"English: {sample_text}\")\nprint(f\"Hungarian: {translated_text}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:49:29.056893Z","iopub.execute_input":"2025-03-14T16:49:29.057219Z","iopub.status.idle":"2025-03-14T16:49:29.277836Z","shell.execute_reply.started":"2025-03-14T16:49:29.057197Z","shell.execute_reply":"2025-03-14T16:49:29.277108Z"}},"outputs":[{"name":"stdout","text":"English: I love books.\nHungarian: Szeretem kÃ¶nyveket.\n","output_type":"stream"}],"execution_count":11},{"id":"05e2725e-0f18-49c0-8156-e91f178b2295","cell_type":"code","source":"# Save the fine-tuned model and tokenizer to the specified directory\noutput_dir = \"/kaggle/working/t5-small-hungarian-translator\"\n\n# Create directory if it doesn't exist\nimport os\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\n# Save the model\nfinal_model.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\n\nprint(f\"Model and tokenizer successfully saved to {output_dir}\")\n\n# If you want to download the files to your local machine\n# You'll need to zip the directory first\n!tar -czvf /kaggle/working/t5-small-hungarian-translator.tar.gz -C /kaggle/working t5-small-hungarian-translator","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T17:03:08.113627Z","iopub.execute_input":"2025-03-14T17:03:08.113999Z","iopub.status.idle":"2025-03-14T17:03:22.238105Z","shell.execute_reply.started":"2025-03-14T17:03:08.113973Z","shell.execute_reply":"2025-03-14T17:03:22.236944Z"}},"outputs":[{"name":"stdout","text":"Model and tokenizer successfully saved to /kaggle/working/t5-small-hungarian-translator\nt5-small-hungarian-translator/\nt5-small-hungarian-translator/tokenizer_config.json\nt5-small-hungarian-translator/special_tokens_map.json\nt5-small-hungarian-translator/model.safetensors\nt5-small-hungarian-translator/config.json\nt5-small-hungarian-translator/spiece.model\nt5-small-hungarian-translator/generation_config.json\nt5-small-hungarian-translator/tokenizer.json\n","output_type":"stream"}],"execution_count":12},{"id":"f3daf289-e653-46b8-845f-f45f96129636","cell_type":"code","source":"!zip -r tensorboard_logs.zip /kaggle/working/t5-small-translation-tensorboard\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T17:12:39.787011Z","iopub.execute_input":"2025-03-14T17:12:39.787352Z","iopub.status.idle":"2025-03-14T17:12:40.024905Z","shell.execute_reply.started":"2025-03-14T17:12:39.787325Z","shell.execute_reply":"2025-03-14T17:12:40.023841Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/t5-small-translation-tensorboard/ (stored 0%)\n  adding: kaggle/working/t5-small-translation-tensorboard/events.out.tfevents.1741954390.aba8776d12aa.31.1 (deflated 25%)\n  adding: kaggle/working/t5-small-translation-tensorboard/events.out.tfevents.1741958188.aba8776d12aa.31.6 (deflated 62%)\n  adding: kaggle/working/t5-small-translation-tensorboard/events.out.tfevents.1741962446.aba8776d12aa.31.9 (deflated 25%)\n  adding: kaggle/working/t5-small-translation-tensorboard/events.out.tfevents.1741958187.aba8776d12aa.31.5 (deflated 25%)\n  adding: kaggle/working/t5-small-translation-tensorboard/events.out.tfevents.1741959522.aba8776d12aa.31.7 (deflated 25%)\n  adding: kaggle/working/t5-small-translation-tensorboard/events.out.tfevents.1741953248.aba8776d12aa.31.0 (deflated 61%)\n  adding: kaggle/working/t5-small-translation-tensorboard/events.out.tfevents.1741962447.aba8776d12aa.31.10 (deflated 62%)\n  adding: kaggle/working/t5-small-translation-tensorboard/events.out.tfevents.1741954392.aba8776d12aa.31.2 (deflated 65%)\n  adding: kaggle/working/t5-small-translation-tensorboard/events.out.tfevents.1741959524.aba8776d12aa.31.8 (deflated 66%)\n  adding: kaggle/working/t5-small-translation-tensorboard/events.out.tfevents.1741956852.aba8776d12aa.31.4 (deflated 62%)\n  adding: kaggle/working/t5-small-translation-tensorboard/events.out.tfevents.1741956851.aba8776d12aa.31.3 (deflated 25%)\n  adding: kaggle/working/t5-small-translation-tensorboard/events.out.tfevents.1741963494.aba8776d12aa.31.11 (deflated 68%)\n","output_type":"stream"}],"execution_count":13},{"id":"c2d02769-5e3f-4129-9bb0-8be9a9add299","cell_type":"code","source":"from IPython.display import FileLink\n\nFileLink(\"tensorboard_logs.zip\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T17:13:14.268900Z","iopub.execute_input":"2025-03-14T17:13:14.269222Z","iopub.status.idle":"2025-03-14T17:13:14.275591Z","shell.execute_reply.started":"2025-03-14T17:13:14.269196Z","shell.execute_reply":"2025-03-14T17:13:14.274762Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/tensorboard_logs.zip","text/html":"<a href='tensorboard_logs.zip' target='_blank'>tensorboard_logs.zip</a><br>"},"metadata":{}}],"execution_count":14},{"id":"13aec6e7-a441-4b02-b70b-502e8a3cbe09","cell_type":"code","source":"!zip -r optuna_logs.zip /kaggle/working/t5-small-translation-optuna\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T17:18:10.978911Z","iopub.execute_input":"2025-03-14T17:18:10.979248Z","iopub.status.idle":"2025-03-14T17:19:22.490250Z","shell.execute_reply.started":"2025-03-14T17:18:10.979224Z","shell.execute_reply":"2025-03-14T17:19:22.489321Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/t5-small-translation-optuna/ (stored 0%)\n  adding: kaggle/working/t5-small-translation-optuna/checkpoint-6857/ (stored 0%)\n  adding: kaggle/working/t5-small-translation-optuna/checkpoint-6857/tokenizer_config.json (deflated 95%)\n  adding: kaggle/working/t5-small-translation-optuna/checkpoint-6857/special_tokens_map.json (deflated 85%)\n  adding: kaggle/working/t5-small-translation-optuna/checkpoint-6857/scheduler.pt (deflated 55%)\n  adding: kaggle/working/t5-small-translation-optuna/checkpoint-6857/model.safetensors (deflated 8%)\n  adding: kaggle/working/t5-small-translation-optuna/checkpoint-6857/rng_state.pth (deflated 25%)\n  adding: kaggle/working/t5-small-translation-optuna/checkpoint-6857/config.json (deflated 62%)\n  adding: kaggle/working/t5-small-translation-optuna/checkpoint-6857/training_args.bin (deflated 51%)\n  adding: kaggle/working/t5-small-translation-optuna/checkpoint-6857/trainer_state.json (deflated 76%)\n  adding: kaggle/working/t5-small-translation-optuna/checkpoint-6857/spiece.model (deflated 48%)\n  adding: kaggle/working/t5-small-translation-optuna/checkpoint-6857/optimizer.pt (deflated 7%)\n  adding: kaggle/working/t5-small-translation-optuna/checkpoint-6857/generation_config.json (deflated 29%)\n  adding: kaggle/working/t5-small-translation-optuna/checkpoint-6857/tokenizer.json (deflated 74%)\n  adding: kaggle/working/t5-small-translation-optuna/checkpoint-500/ (stored 0%)\n  adding: kaggle/working/t5-small-translation-optuna/checkpoint-500/tokenizer_config.json (deflated 95%)\n  adding: kaggle/working/t5-small-translation-optuna/checkpoint-500/special_tokens_map.json (deflated 85%)\n  adding: kaggle/working/t5-small-translation-optuna/checkpoint-500/scheduler.pt (deflated 56%)\n  adding: kaggle/working/t5-small-translation-optuna/checkpoint-500/model.safetensors (deflated 9%)\n  adding: kaggle/working/t5-small-translation-optuna/checkpoint-500/rng_state.pth (deflated 25%)\n  adding: kaggle/working/t5-small-translation-optuna/checkpoint-500/config.json (deflated 62%)\n  adding: kaggle/working/t5-small-translation-optuna/checkpoint-500/training_args.bin (deflated 51%)\n  adding: kaggle/working/t5-small-translation-optuna/checkpoint-500/trainer_state.json (deflated 62%)\n  adding: kaggle/working/t5-small-translation-optuna/checkpoint-500/spiece.model (deflated 48%)\n  adding: kaggle/working/t5-small-translation-optuna/checkpoint-500/optimizer.pt (deflated 7%)\n  adding: kaggle/working/t5-small-translation-optuna/checkpoint-500/generation_config.json (deflated 29%)\n  adding: kaggle/working/t5-small-translation-optuna/checkpoint-500/tokenizer.json (deflated 74%)\n","output_type":"stream"}],"execution_count":15},{"id":"4ffffc1e-eddc-4dbe-8a84-32dcd96cceb9","cell_type":"code","source":"from IPython.display import FileLink\n\nFileLink(\"optuna_logs.zip\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T17:19:22.491874Z","iopub.execute_input":"2025-03-14T17:19:22.492217Z","iopub.status.idle":"2025-03-14T17:19:22.497942Z","shell.execute_reply.started":"2025-03-14T17:19:22.492183Z","shell.execute_reply":"2025-03-14T17:19:22.497201Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/optuna_logs.zip","text/html":"<a href='optuna_logs.zip' target='_blank'>optuna_logs.zip</a><br>"},"metadata":{}}],"execution_count":16}]}